{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch Model on Streaming Data with Spark Streaming and Online  Predictions with Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark was build to analyze Big Data with faster speed. One of the important features that Apache Spark offers is the ability to run the computations in memory. \n",
    "PySpark is the interface that gives access to Spark using the Python programming language. PySpark is an API developed in python for spark programming and writing spark applications in Python style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this effort, we will mostly deal with the PySpark ml - machine learning library that can be used to import the Logistic Regression model or other machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab?\n",
    "Colab by Google is based on Jupyter Notebook which is an incredibly powerful tool that leverages google docs features. Since it runs on google server, we don't need to install anything in our system locally, be it Spark or deep learning model. The most attractive features of Colab are the free GPU and TPU support! Since the GPU support runs on Google's own server, it is, in fact, faster than some commercially available GPUs like the Nvidia 1050Ti. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s create a simple logistic regression model with PySpark in Google Colab. To open Colab Jupyter Notebook, click on this link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Pyspark in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: apt-get: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Apache Hadoop 3.x now supports only Java 8\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Colab is ready to run PySpark. Let's build a simple Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# location to stream train data from \n",
    "train_csv_dir = os.getcwd()+\"/module11_cs1/train_data_csv\"\n",
    "# location to stream test data from\n",
    "test_csv_dir = os.getcwd()+\"/module11_cs1/test_data_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing dirs if they do not exist\n",
    "if not os.path.exists(train_csv_dir):\n",
    "    os.makedirs(train_csv_dir)\n",
    "\n",
    "if not os.path.exists(test_csv_dir):\n",
    "    os.makedirs(test_csv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Batch Logistic Regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates how to load training and testing data from two different input streams of text files, parse the streams, fit a logistic regression model online to the first stream, and make predictions on the second stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we register the streams for training and testing and start the job.\n",
    "\n",
    "We can now save text files with data to the training or testing folders. Anytime a text file is placed in sys.argv[1] the model will update. Anytime a text file is placed in sys.argv[2] you will see predictions. As you feed more data to the training directory, the predictions theoretically will get better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model in time intervals using Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Spark Streaming application running on a cluster to be stable, the system should be able to process data as fast as it is being received, so the batch processing time should be less than the batch interval.\n",
    "The batch interval needs to be set such that the expected data rate in production can be sustained.\n",
    "We will test it with a conservative batch interval (say, 5-20 seconds) and a low data rate. To verify whether the system is able to keep up with the data rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to create and setup a new StreamingContext\n",
    "def functionToCreateContext():\n",
    "    global spark\n",
    "    global dataset_df\n",
    "    global userSchema\n",
    "    global train_duration\n",
    "    \n",
    "    sc = SparkContext(\"local[2]\", \"Batch_Model_on_Stream_Data\")  # new context\n",
    "    ssc = StreamingContext(sc, train_duration)\n",
    "    \n",
    "    spark = SparkSession(sc)\n",
    "    sqlContext = SQLContext(sc)\n",
    "    # create an empty datframe\n",
    "    dataset_df = sqlContext.createDataFrame(sc.emptyRDD(), userSchema)\n",
    "\n",
    "    emptly_stream = ssc.queueStream([dataset_df.limit(1).rdd], oneAtATime=True, default=dataset_df.limit(1).rdd)  # create DStream\n",
    "    emptly_stream.foreachRDD(train)\n",
    "    ssc.start() \n",
    "\n",
    "    ssc.checkpoint(checkpointDirectory)  # set checkpoint directory\n",
    "    return ssc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(df, epoch_id):\n",
    "    global model\n",
    "    \n",
    "    if not model == None:\n",
    "        print(\"Predictions:\")\n",
    "        predictions = model.transform(df)['spam', 'message','label', 'prediction', 'probability']\n",
    "        # print predictions to the console\n",
    "        predictions.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Model has not seen training data yet, therefore - no model exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendPartition(df, epoch_id):\n",
    "    global dataset_df\n",
    "    \n",
    "    if not len(df.head(1)) == 0:\n",
    "        dataset_df = df.union(dataset_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(rdd):\n",
    "    global dataset_df\n",
    "    global model\n",
    "    global prev_length\n",
    "    global evaluate\n",
    "    global crossval_full\n",
    "\n",
    "    if dataset_df.count() > prev_length:\n",
    "        prev_length = dataset_df.count()\n",
    "\n",
    "        if evaluate == True:\n",
    "            # Split to train and test\n",
    "            (trainingData, testData) = dataset_df.randomSplit([0.7, 0.3], seed=0)\n",
    "        else:\n",
    "            trainingData = dataset_df\n",
    "\n",
    "        my_train_df = trainingData\n",
    "\n",
    "        print(\"\\n\\nStarting to fit a model on \" + str(my_train_df.count()) +\" records\")\n",
    "        # crossvalidating on full_pipeline\n",
    "        model = crossval_full.fit(my_train_df)\n",
    "        print(\"Model fit compleeted\\n\")\n",
    "        \n",
    "        if evaluate == True:\n",
    "            predictions = model.transform(testData)\n",
    "            evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "            accuracy = evaluator.evaluate(predictions)\n",
    "            print(\"Evalluated on \"+ str(predictions.count()) +\" records\")\n",
    "            print (\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "#userSchema = StructType().add(\"spam\", \"string\").add(\"message\", \"string\")\n",
    "#df = spark \\\n",
    "#    .readStream \\\n",
    "#    .format(\"socket\") \\\n",
    "#    .option(\"host\", \"localhost\") \\\n",
    "#    .option(\"port\", 9999) \\\n",
    "#    .load()\n",
    "#df = df.selectExpr( \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "#import pyspark.sql.functions as f\n",
    "# split text lines into two fields\n",
    "#split_col = f.split(df['value'], '\\t')\n",
    "#df = df.withColumn('spam', split_col.getItem(0))\n",
    "#df = df.withColumn('message', split_col.getItem(1))\n",
    "#df = df.select(['spam', 'message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n",
    "userSchema = StructType().add(\"spam\", \"string\").add(\"message\", \"string\")\n",
    "\n",
    "dataset_df = None\n",
    "# alternatilvelly, start with an intitial dataset\n",
    "# dataset_df = sc.textFile(\"gs://drive3/data/spark/8_cs1_dataset/SMSSpamCollection\").map(lambda line: re.split('\\t', line)).toDF([\"spam\", \"message\"])  \n",
    "\n",
    "checkpointDirectory = \"chkpnt\"\n",
    "\n",
    "spark = None\n",
    "\n",
    "model = None\n",
    "\n",
    "prev_length = 0\n",
    "\n",
    "evaluate = True\n",
    "# duration of training a model on entire batch dataset\n",
    "train_duration = 20  # train a model every n seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get StreamingContext from checkpoint data or create a new one\n",
    "context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)\n",
    "\n",
    "# Start the context\n",
    "#context.start()\n",
    "#context.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "# Extract words\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "# Remove custom stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "# create features\n",
    "hashingTF = HashingTF(numFeatures=10, inputCol=\"filtered\", outputCol=\"features\")\n",
    "rf = RandomForestClassifier().setFeaturesCol(\"features\").setNumTrees(10)\n",
    "#dt = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "full_pipeline = Pipeline().setStages([ indexer, tokenizer, remover, hashingTF, lr])\n",
    "############################################################\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    ".addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    ".build()\n",
    "\n",
    "\n",
    "crossval_full = CrossValidator(estimator=full_pipeline,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=BinaryClassificationEvaluator(),\n",
    "                            numFolds=2)  # use 3+ folds in practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream processing on Sprak SQL engine is fast scalable, fault-tolerant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to fit a model on 3920 records\n",
      "Model fit compleeted\n",
      "\n",
      "Evalluated on 1654 records\n",
      "Accuracy 0.8891976062530532\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# screen subfolders in working directory for new csv files\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .schema(userSchema)  \\\n",
    "    .csv(train_csv_dir) \\\n",
    "    .writeStream.foreachBatch(sendPartition)\\\n",
    "    .start()\n",
    "\n",
    "time.sleep(train_duration*4)\n",
    "\n",
    "df_test = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .schema(userSchema)  \\\n",
    "    .csv(test_csv_dir) \\\n",
    "    .writeStream.foreachBatch(predict).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "+----+--------------------+-----+----------+--------------------+\n",
      "|spam|             message|label|prediction|         probability|\n",
      "+----+--------------------+-----+----------+--------------------+\n",
      "| ham|Go until jurong p...|  0.0|       0.0|[0.98222810319386...|\n",
      "| ham|Ok lar... Joking ...|  0.0|       0.0|[0.98225861109283...|\n",
      "|spam|Free entry in 2 a...|  1.0|       1.0|[0.01885752181723...|\n",
      "| ham|U dun say so earl...|  0.0|       0.0|[0.97771260955088...|\n",
      "| ham|Nah I don't think...|  0.0|       0.0|[0.94934090451217...|\n",
      "|spam|FreeMsg Hey there...|  1.0|       1.0|[0.37203591496283...|\n",
      "| ham|Even my brother i...|  0.0|       0.0|[0.96752277542991...|\n",
      "| ham|As per your reque...|  0.0|       0.0|[0.94607020920184...|\n",
      "|spam|WINNER!! As a val...|  1.0|       1.0|[0.05238823021501...|\n",
      "|spam|Had your mobile 1...|  1.0|       1.0|[0.06098116990971...|\n",
      "| ham|I'm gonna be home...|  0.0|       0.0|[0.99097672028864...|\n",
      "|spam|SIX chances to wi...|  1.0|       1.0|[0.12092943424657...|\n",
      "|spam|URGENT! You have ...|  1.0|       1.0|[0.00643803948809...|\n",
      "| ham|I've been searchi...|  0.0|       0.0|[0.76008412511481...|\n",
      "| ham|I HAVE A DATE ON ...|  0.0|       0.0|[0.98464241880313...|\n",
      "|spam|XXXMobileMovieClu...|  1.0|       0.0|[0.54231596261901...|\n",
      "| ham|Oh k...i'm watchi...|  0.0|       0.0|[0.97176495772320...|\n",
      "| ham|Eh u remember how...|  0.0|       0.0|[0.95957281852508...|\n",
      "| ham|Fine if thatÂ’s th...|  0.0|       0.0|[0.97648397550667...|\n",
      "|spam|England v Macedon...|  1.0|       1.0|[0.18686282916411...|\n",
      "+----+--------------------+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ssc.start() \n",
    "# Start the computation\n",
    "context.awaitTermination()  # Wait for the computation to terminate\n",
    "#print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flume-ng agent --conf conf --conf-file /usr/local/Cellar/flume/1.9.0/libexec/conf/flume-sample.conf  -Dflume.root.logger=DEBUG,console --name a1 -Xmx512m -Xms256m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!flume-ng agent --conf conf --conf-file /Users/val/Documents/code/spark/m11_to_Upload/netcat.conf.txt  -Dflume.root.logger=DEBUG,console --name NetcatAgent -Xmx512m -Xms256m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic\n",
    "#df = spark \\\n",
    "#    .readStream \\\n",
    "#    .format(\"kafka\") \\\n",
    "#    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#    .option(\"subscribe\", \"sample-topic\") \\\n",
    "#    .option(\"startingOffsets\", \"earliest\") \\\n",
    "#    .load()\n",
    "\n",
    "#df = df.selectExpr( \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "#import pyspark.sql.functions as f\n",
    "# split text lines into two fields\n",
    "#split_col = f.split(df['value'], '\\t')\n",
    "#df = df.withColumn('spam', split_col.getItem(0))\n",
    "#df = df.withColumn('message', split_col.getItem(1))\n",
    "#df = df.select(['spam', 'message'])\n",
    "\n",
    "# convert label: spam = 1, ham = 0 \n",
    "#from pyspark.sql import functions as f\n",
    "#df = df.withColumn(\"label\", f.when(f.col('spam') == \"spam\", 1).otherwise(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
